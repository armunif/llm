{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c1d88c-d1ec-4bbf-99be-9bab5c430b7d",
   "metadata": {},
   "source": [
    "### Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6066e0a-8abf-44a8-87cd-5ecd2eb3c721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through th\n"
     ]
    }
   ],
   "source": [
    "with open (\"verdicttext/the-verdict.txt\", \"r\", encoding =\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cc1df-76b1-4fb9-8c8f-d098cad905e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6115722a-cc4d-4877-b06d-5f2c7cea148e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'tokenization', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a tokenization test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a61dfe04-18d4-4309-946f-067c41448b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ', ', 'world', '. ', 'This', ', ', 'is', ' ', 'a', ' ', 'tokenization', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.] |\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02659664-d167-4c15-8b13-c722e42822dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ', ', 'world', '. ', 'This', ', ', 'is', 'a', 'tokenization', 'test.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b938744c-3dca-4312-9567-f68df0b26479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify it a bit further so that it can also handle other types of punctuation, such as question\n",
    "# marks, quotation marks, and the double-dashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f81791f-fd25-4e71-a072-05a07717ff09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'tokenization', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a tokenization test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87c8dc-dd5d-4da6-93b6-3aa4e14cc992",
   "metadata": {},
   "source": [
    "### Apply the tokenization to our sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527122d9-8529-4aee-94fd-cd7086604b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5172e0d0-b647-4de5-b735-f35f1bfacbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3cd4c-0dfe-4217-87a0-6008e7a6fc8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Convert tokens into token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f247b19-60f8-437e-881d-0db272b48270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting these tokens from a Python string to an integer representation to\n",
    "# produce the token IDs. This conversion is an intermediate step before converting the\n",
    "# token IDs into embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb13bcc-c5d4-4523-ab8f-f2e2b1bb2b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b647b0-6a90-43fe-9fa2-4e42d6d465ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbb708fb-6af6-40b9-bd0d-212f28d12640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implement a complete tokenizer class in Python with an encode method that\n",
    "# splits text into tokens and carries out the string-to-integer mapping to produce token\n",
    "# IDs via the vocabulary. In addition, we’ll implement a decode method that carries out\n",
    "# the reverse integer-to-string mapping to convert the token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9400db06-f617-4272-b529-f04a3c1acc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "928eae9a-3d9b-4474-9c71-0ac26654ffe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer\n",
    "# objects via an existing vocabulary, which we can then use to encode and decode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f4cbe88-d54e-4a6e-bd4e-24f186880a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ced0c19e-a469-4228-a23f-29e574083896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let’s see whether we can turn these token IDs back into text using the decode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5d50dd6-dac1-4a67-814a-f29824a2f4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5984e11-e128-4ed0-825f-4840d64097a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let’s now apply it to a new text sample not contained in the training set\n",
    "\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d010ca08-ce4d-47d0-a72f-d75db381b1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will receive a Key Error in the previous cell because the word “Hello” was not used in the our sample training text dataset i.e. the-verdict.txt\n",
    "# this is why we need to consider large and diverse training sets to extend the vocabulary when working on LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86128966-b395-4cac-a132-b96951418dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to modify the tokenizer to handle unknown words. We also need to address\n",
    "# the usage and addition of special context tokens that can enhance a model’s understanding\n",
    "# of context or other relevant information in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cef74ca-58fa-4373-9e42-c21a4bcec920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These special tokens can include markers for unknown words and document boundaries\n",
    "# we will modify the vocabulary and tokenizer, SimpleTokenizerV2, to support\n",
    "# two new tokens, <|unk|> and <|endoftext|>\n",
    "# the <|unk|> token will be used if it encounters a word that is not part of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6eb51ee-6add-430a-a907-fd7086cfbb44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# when training GPT-like LLMs on multiple independent documents or books\n",
    "# it is common to insert a <|endoftext|> before each document or book that follows a previous text source\n",
    "# This helps the LLM understand that although these text sources are concatenated for training, \n",
    "# they are, in fact, unrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a0a49cb-b6ce-4e2f-bbf8-317c7eab51e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "#Let’s now modify the vocabulary to include these two special tokens, <unk> and\n",
    "# <|endoftext|>, by adding them to our list of all unique words\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f27ed311-191e-4f2f-81a3-296cbaa227b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# quick check\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681e946-a079-4f4c-98f4-48e490782fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7e57670-5d0d-4ad5-a70e-fef0edca857f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    "        \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        \n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text                     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bd38a-9d16-44be-b6d8-f6450625b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to the SimpleTokenizerV1 we implemented in listing 2.3, the new Simple-\n",
    "# TokenizerV2 replaces unknown words with <|unk|> tokens\n",
    "# Let’s now try this new tokenizer out in practice. For this, we will use a simple text\n",
    "# sample that we concatenate from two independent and unrelated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e23e9d96-65a7-4650-8838-3a59675866fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13e016-d13c-4d90-929b-50d0e6a98729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let’s tokenize the sample text using the SimpleTokenizerV2 on the vocab we previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b227b76-4ee1-4f5c-8351-50d68f8f3f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c06e12f-54d0-47e7-a154-76654f6866d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# decode\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc7058-f8a3-4ec8-99a7-5ef66acb03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on comparing this detokenized text above with the original input text, we know that\n",
    "# the training dataset,the-verdict.txt, does not contain the\n",
    "# words “Hello” and “palace.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2a46a-94c8-4921-becb-bd6144093d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the LLM, some researchers also consider additional special tokens\n",
    "# such as the following:\n",
    "\n",
    "# [BOS] (beginning of sequence)—This token marks the start of a text.It signifies to the LLM where a piece of content begins.\n",
    "\n",
    "# [EOS] (end of sequence)—This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to\n",
    "# <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins.\n",
    "\n",
    "# [PAD] (padding)—When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. \n",
    "# To ensure all texts have the same length, the shorter texts are extended or “padded” using the [PAD] token, up to\n",
    "# the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a094b-9e35-42b9-a5a5-6c0020e00858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer used for GPT models does not need any of these tokens; it only uses an\n",
    "# <|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token.\n",
    "# <|endoftext|> is also used for padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf524d-2f15-4dcb-9626-e968fe0c8235",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e14d92d8-611d-42fa-ac9d-2a5058a3a15a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://GB-SVC-DSW-NEXUS:****@nexus302.systems.uk.hsbc:8081/nexus/repository/pypi-proxy_n3p/simple\n",
      "Collecting tiktoken\n",
      "  Downloading https://nexus302.systems.uk.hsbc:8081/nexus/repository/pypi-proxy_n3p/packages/tiktoken/0.9.0/tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/conda/miniconda3/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/miniconda3/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c180ea9c-8428-4bb3-aaf3-ee4c0209626c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64a40cc5-eeb6-4203-b2cb-aa7c70237551",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown encoding tiktoken_cache/9b5ad71b2ce5302211f9c61530b329a4922fc6a4.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.9.0 (are you on latest?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiktoken_cache/9b5ad71b2ce5302211f9c61530b329a4922fc6a4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/tiktoken/registry.py:79\u001b[0m, in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ENCODING_CONSTRUCTORS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENCODING_CONSTRUCTORS:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiktoken version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (are you on latest?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m constructor \u001b[38;5;241m=\u001b[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[1;32m     86\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encoding(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstructor())\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown encoding tiktoken_cache/9b5ad71b2ce5302211f9c61530b329a4922fc6a4.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.9.0 (are you on latest?)"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822fb40f-8eda-406a-884d-2227924ca135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
